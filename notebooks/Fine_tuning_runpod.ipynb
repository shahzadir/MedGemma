{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "445e7bc9-e783-48e6-9f09-3aa295c1d216",
   "metadata": {},
   "source": [
    "## Installing Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8ceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is for runpod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1124aadd-600c-4ff3-88b8-db724d3a8071",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade --quiet transformers bitsandbytes accelerate 'datasets==3.4.1' evaluate peft trl scikit-learn kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b02a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUSH_MODEL_ID = \"shahzira/appendix-model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b4bee-6824-4c16-ac55-30285152a199",
   "metadata": {},
   "source": [
    "## Loading and Processing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dbe2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "def flatten_dataset_structure(src_dir, dst_dir):\n",
    "    os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "    for label in ['positive', 'negative']:\n",
    "        label_src = os.path.join(src_dir, label)\n",
    "        label_dst = os.path.join(dst_dir, label)\n",
    "        os.makedirs(label_dst, exist_ok=True)\n",
    "\n",
    "        for case in os.listdir(label_src):\n",
    "            case_path = os.path.join(label_src, case)\n",
    "            if os.path.isdir(case_path):\n",
    "                for img_file in os.listdir(case_path):\n",
    "                    src_img_path = os.path.join(case_path, img_file)\n",
    "                    dst_img_path = os.path.join(label_dst, f\"{case}_{img_file}\")\n",
    "                    shutil.copy2(src_img_path, dst_img_path)\n",
    "\n",
    "# Example: Flatten all three splits\n",
    "base_dir = r\"E:\\Appendicitis_final_materials\\Data\\Imaging\\Repeat2_Fold3_data_cubic\"\n",
    "\n",
    "flatten_dataset_structure(\n",
    "    src_dir=os.path.join(base_dir, \"training_data_png\"),\n",
    "    dst_dir=os.path.join(base_dir, \"flattened_training_data\")\n",
    ")\n",
    "flatten_dataset_structure(\n",
    "    src_dir=os.path.join(base_dir, \"validation_data_png\"),\n",
    "    dst_dir=os.path.join(base_dir, \"flattened_validation_data\")\n",
    ")\n",
    "flatten_dataset_structure(\n",
    "    src_dir=os.path.join(base_dir, \"test_data_png\"),\n",
    "    dst_dir=os.path.join(base_dir, \"flattened_test_data\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8666b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Paths to the already processed PNG folders\n",
    "data_dir = r\"E:\\Appendicitis_final_materials\\Data\\Imaging\\Repeat2_Fold3_data_cubic\"\n",
    "train_dir = f\"{data_dir}/flattened_training_data\"\n",
    "val_dir = f\"{data_dir}/flattened_validation_data\"\n",
    "test_dir = f\"{data_dir}/flattened_test_data\"\n",
    "\n",
    "# Load each split individually\n",
    "train_ds = load_dataset(\"imagefolder\", data_dir=train_dir, split=\"train\")\n",
    "val_ds = load_dataset(\"imagefolder\", data_dir=val_dir, split=\"train\")      # Note: 'train' here means all images in that folder\n",
    "test_ds = load_dataset(\"imagefolder\", data_dir=test_dir, split=\"train\")\n",
    "\n",
    "# Combine into a DatasetDict\n",
    "data = DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"validation\": val_ds,\n",
    "    \"test\": test_ds,\n",
    "})\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a77bda-3152-4d97-b66b-fe2cddd85a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first image (as a PIL object)\n",
    "data[\"train\"][0][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85470739-3559-406b-af67-07a6d2ccbc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHOLOGY_CLASSES = data[\"train\"].features[\"label\"].names\n",
    "print(\"Detected classes:\", PATHOLOGY_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8048ede-ccc5-4b88-b8bf-e3e5d4c2ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHOLOGY_CLASSES = ['A: normal', 'B: appendix']\n",
    "\n",
    "options = \"\\n\".join(PATHOLOGY_CLASSES)\n",
    "PROMPT = f\"What is the most likely type of pathology shown in the CT image?\\n{options}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e7b335-2b7c-40d9-96e8-c7c7d568cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(example: dict[str, any]) -> dict[str, any]:\n",
    "    example[\"messages\"] = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": PROMPT,\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": PATHOLOGY_CLASSES[example[\"label\"]],\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    return example\n",
    "\n",
    "# Apply the formatting to the dataset\n",
    "formatted_data = data.map(format_data)\n",
    "\n",
    "# Display a sample formatted data point\n",
    "formatted_data[\"train\"][0][\"messages\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f258bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_data[\"train\"][20090][\"messages\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b4731-e9f5-4ba2-bd23-ff6cb3ac7c50",
   "metadata": {},
   "source": [
    "## Loading the Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1814c228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import whoami\n",
    "print(whoami())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d501658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "try:\n",
    "    info = api.model_info(\"google/medgemma-4b-it\")\n",
    "    print(\"Access granted!\")\n",
    "except Exception as e:\n",
    "    print(f\"Access denied: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b40bcf-6e10-458c-9712-b8425457dcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"google/medgemma-4b-it\"\n",
    "\n",
    "# Check if GPU supports bfloat16\n",
    "if torch.cuda.get_device_capability()[0] < 8:\n",
    "    raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")\n",
    "\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Use right padding to avoid issues during training\n",
    "processor.tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39285917-2c44-46bf-a052-145446d0967a",
   "metadata": {},
   "source": [
    "## Setting up the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30020ce0-cde2-4f1f-9a0d-659f6953ce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\n",
    "        \"lm_head\",\n",
    "        \"embed_tokens\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03e219b-6228-4516-9e1b-0593a6cfefb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples: list[dict[str, any]]):\n",
    "    texts = []\n",
    "    images = []\n",
    "    for example in examples:\n",
    "        images.append([example[\"image\"]])\n",
    "        texts.append(\n",
    "            processor.apply_chat_template(\n",
    "                example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "            ).strip()\n",
    "        )\n",
    "\n",
    "    # Tokenize the texts and process the images\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # The labels are the input_ids, with the padding and image tokens masked in\n",
    "    # the loss computation\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "    # Mask image tokens\n",
    "    image_token_id = [\n",
    "        processor.tokenizer.convert_tokens_to_ids(\n",
    "            processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
    "        )\n",
    "    ]\n",
    "    # Mask tokens that are not used in the loss computation\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == image_token_id] = -100\n",
    "    labels[labels == 262144] = -100\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d42066d-7d0f-45ba-b410-574ca30f3bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"medgemma-brain-cancer\",            \n",
    "    num_train_epochs=1,                       \n",
    "    per_device_train_batch_size=8,                           \n",
    "    per_device_eval_batch_size=8,                            \n",
    "    gradient_accumulation_steps=8,                           \n",
    "    gradient_checkpointing=True,                             \n",
    "    optim=\"adamw_torch_fused\",                               \n",
    "    logging_steps=0.1,                                        \n",
    "    save_strategy=\"epoch\",                                   \n",
    "    eval_strategy=\"steps\",                                   \n",
    "    eval_steps=0.1,                                           \n",
    "    learning_rate=2e-4,                             \n",
    "    bf16=True,                                               \n",
    "    max_grad_norm=0.3,                                       \n",
    "    warmup_ratio=0.03,                                       \n",
    "    lr_scheduler_type=\"linear\",                              \n",
    "    push_to_hub=True,                                        \n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  \n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},           \n",
    "    remove_unused_columns = False,                           \n",
    "    label_names=[\"labels\"],                                  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a6cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "from peft import PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e4e881-467e-4804-9ef1-3706ed85b7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=formatted_data[\"train\"],\n",
    "    eval_dataset=formatted_data[\"validation\"].shuffle().select(range(50)), \n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec612ba-9006-484c-bfeb-77d2d03eade5",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6f3de5-df14-432b-890b-a0467087ef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e25489-48cd-4152-b762-2c58572f9489",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()\n",
    "processor.save_pretrained(PUSH_MODEL_ID)\n",
    "processor.push_to_hub(PUSH_MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e4751d-1958-4ae8-948b-c50c429e79ce",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory to avoid OOM errors\n",
    "import torch\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c57d34-fdbe-4b3e-90ea-5042bb612d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import libraries\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor, GenerationConfig\n",
    "from datasets import ClassLabel\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, recall_score, confusion_matrix\n",
    "from typing import List, Any, Callable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fd726e",
   "metadata": {},
   "source": [
    "### Setting up for model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cc7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define function to format data\n",
    "def format_test_data(example: dict[str, any]) -> dict[str, any]:\n",
    "    example[\"messages\"] = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": PROMPT},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths (from your original code)\n",
    "data_dir = r\"E:\\Appendicitis_final_materials\\Data\\Imaging\\Repeat2_Fold3_data_cubic\"\n",
    "train_dir = f\"{data_dir}/flattened_training_data\"\n",
    "val_dir = f\"{data_dir}/flattened_validation_data\"\n",
    "test_dir = f\"{data_dir}/flattened_test_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c506ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format all datasets and cast labels becasue we want to apply trained model on all datasets\n",
    "data_splits = [\"train\", \"validation\", \"test\"]\n",
    "formatted_data = {}\n",
    "for split in data_splits:\n",
    "    formatted_data[split] = data[split].map(format_test_data)\n",
    "    formatted_data[split] = formatted_data[split].cast_column(\n",
    "        \"label\", ClassLabel(names=PATHOLOGY_CLASSES)\n",
    "    )\n",
    "\n",
    "\n",
    "# Define label feature and alternative labels\n",
    "LABEL_FEATURE = formatted_data[\"test\"].features[\"label\"]\n",
    "ALT_LABELS = dict([(label, f\"({label.replace(': ', ') ')}\") for label in PATHOLOGY_CLASSES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0692d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up image paths to be used in excel export\n",
    "image_paths = {\n",
    "    split: [f[\"path\"] for f in data[split].data_files]\n",
    "    for split in data_splits\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255f60a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postprocess function\n",
    "def postprocess(prediction, do_full_match: bool = False) -> int:\n",
    "    if isinstance(prediction, str):\n",
    "        response_text = prediction\n",
    "    else:\n",
    "        response_text = prediction[0][\"generated_text\"]\n",
    "\n",
    "    if do_full_match:\n",
    "        try:\n",
    "            return LABEL_FEATURE.str2int(response_text)\n",
    "        except ValueError:\n",
    "            return -1\n",
    "\n",
    "    for label in PATHOLOGY_CLASSES:\n",
    "        if label in response_text or ALT_LABELS[label] in response_text:\n",
    "            return LABEL_FEATURE.str2int(label)\n",
    "\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76e9f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch prediction function\n",
    "def batch_predict(\n",
    "    prompts,\n",
    "    images,\n",
    "    model,\n",
    "    processor,\n",
    "    postprocess,\n",
    "    *,\n",
    "    batch_size=64,\n",
    "    device=\"cuda\",\n",
    "    dtype=torch.bfloat16,\n",
    "    **gen_kwargs\n",
    "):\n",
    "    preds = []\n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        texts = prompts[i : i + batch_size]\n",
    "        imgs = [[img] for img in images[i : i + batch_size]]\n",
    "        enc = processor(text=texts, images=imgs, padding=True, return_tensors=\"pt\").to(\n",
    "            device, dtype=dtype\n",
    "        )\n",
    "        lens = enc[\"attention_mask\"].sum(dim=1)\n",
    "        with torch.inference_mode():\n",
    "            out = model.generate(\n",
    "                **enc,\n",
    "                disable_compile=True,\n",
    "                **gen_kwargs\n",
    "            )\n",
    "        for seq, ln in zip(out, lens):\n",
    "            ans = processor.decode(seq[ln:], skip_special_tokens=True)\n",
    "            preds.append(postprocess(ans))\n",
    "    return preds\n",
    "\n",
    "# Compute metrics including AUC, Sensitivity, and Specificity\n",
    "def compute_metrics(predictions: list[int], references: list[int]) -> dict[str, float]:\n",
    "    metrics = {}\n",
    "    # Load standard metrics\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    \n",
    "    # Compute accuracy and F1\n",
    "    metrics.update(accuracy_metric.compute(predictions=predictions, references=references))\n",
    "    metrics.update(f1_metric.compute(predictions=predictions, references=references, average=\"weighted\"))\n",
    "    \n",
    "    \n",
    "    # Compute Sensitivity and Specificity\n",
    "    tn, fp, fn, tp = confusion_matrix(references, predictions, labels=[0, 1]).ravel()\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    metrics[\"sensitivity\"] = sensitivity\n",
    "    metrics[\"specificity\"] = specificity\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Save predictions to Excel\n",
    "def save_predictions_to_excel(predictions: list[int], dataset, image_paths: list[str], split_name: str, model_type: str):\n",
    "    pred_labels = [PATHOLOGY_CLASSES[pred] if pred != -1 else \"unknown\" for pred in predictions]\n",
    "    original_labels = [PATHOLOGY_CLASSES[label] for label in dataset[\"label\"]]\n",
    "    df = pd.DataFrame({\n",
    "        \"Image_Path\": image_paths,\n",
    "        \"Prediction\": pred_labels,\n",
    "        \"Original_Label\": original_labels\n",
    "    })\n",
    "    output_file = f\"{model_type}_predictions_{split_name}.xlsx\"\n",
    "    df.to_excel(output_file, index=False)\n",
    "    print(f\"Saved {model_type} predictions for {split_name} split to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233deb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 3: Base Model Evaluation\n",
    "#This cell loads the base model, generates predictions for all splits, and stores them for later use.\n",
    "\n",
    "# Model kwargs\n",
    "model_kwargs = dict(\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
    "\n",
    "# Configure generation\n",
    "gen_cfg = GenerationConfig.from_pretrained(model_id)\n",
    "gen_cfg.update(\n",
    "    do_sample=False,\n",
    "    top_k=None,\n",
    "    top_p=None,\n",
    "    cache_implementation=\"dynamic\"\n",
    ")\n",
    "model.generation_config = gen_cfg\n",
    "\n",
    "# Load processor\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "tok = processor.tokenizer\n",
    "model.config.pad_token_id = tok.pad_token_id\n",
    "model.generation_config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "# Generate predictions for all splits\n",
    "base_predictions = {}\n",
    "for split in data_splits:\n",
    "    prompts = [processor.apply_chat_template(c, add_generation_prompt=True, tokenize=False) \n",
    "               for c in formatted_data[split][\"messages\"]]\n",
    "    images = formatted_data[split][\"image\"]\n",
    "    \n",
    "    predictions = batch_predict(\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        prompts=prompts,\n",
    "        images=images,\n",
    "        batch_size=64,\n",
    "        max_new_tokens=40,\n",
    "        postprocess=postprocess\n",
    "    )\n",
    "    base_predictions[split] = predictions\n",
    "    print(f\"Completed base model predictions for {split} split: {len(predictions)} samples\")\n",
    "\n",
    "# Clear memory\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec1dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 4: Base Model Metrics and Excel Saving\n",
    "#This cell computes metrics and saves predictions to Excel for the base model.\n",
    "# Compute metrics and save predictions for base model\n",
    "base_metrics = {}\n",
    "for split in data_splits:\n",
    "    references = formatted_data[split][\"label\"]\n",
    "    predictions = base_predictions[split]\n",
    "    base_metrics[split] = compute_metrics(predictions, references)\n",
    "    print(f\"Base model metrics for {split} split: {base_metrics[split]}\")\n",
    "    save_predictions_to_excel(predictions, formatted_data[split], image_paths[split], split, \"base_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d64300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 5: Fine-Tuned Model Evaluation\n",
    "#This cell loads the fine-tuned model and generates predictions for all splits.\n",
    "# Load fine-tuned model's processor first to avoid OOM errors\n",
    "processor = AutoProcessor.from_pretrained(args.output_dir)\n",
    "tok = processor.tokenizer\n",
    "\n",
    "# Load fine-tuned model\n",
    "model = AutoModelForImageTextToText.from_pretrained(args.output_dir, **model_kwargs)\n",
    "model.generation_config = gen_cfg\n",
    "model.config.pad_token_id = tok.pad_token_id\n",
    "model.generation_config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "# Generate predictions for all splits\n",
    "fine_tuned_predictions = {}\n",
    "for split in data_splits:\n",
    "    prompts = [processor.apply_chat_template(c, add_generation_prompt=True, tokenize=False) \n",
    "               for c in formatted_data[split][\"messages\"]]\n",
    "    images = formatted_data[split][\"image\"]\n",
    "    \n",
    "    predictions = batch_predict(\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        prompts=prompts,\n",
    "        images=images,\n",
    "        batch_size=64,\n",
    "        max_new_tokens=40,\n",
    "        postprocess=postprocess\n",
    "    )\n",
    "    fine_tuned_predictions[split] = predictions\n",
    "    print(f\"Completed fine-tuned model predictions for {split} split: {len(predictions)} samples\")\n",
    "\n",
    "# Clear memory\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8702694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 6: Fine-Tuned Model Metrics and Excel Saving\n",
    "#This cell computes metrics and saves predictions to Excel for the fine-tuned model.\n",
    "# Compute metrics and save predictions for fine-tuned model\n",
    "fine_tuned_metrics = {}\n",
    "for split in data_splits:\n",
    "    references = formatted_data[ \"split\"]][\"label\"]\n",
    "    predictions = fine_tuned_predictions[split]\n",
    "    fine_tuned_metrics[split] = compute_metrics(predictions, references)\n",
    "    print(f\"Fine-tuned model metrics for {split} split: {fine_tuned_metrics[split]}\")\n",
    "    save_predictions_to_excel(predictions, formatted_data[split], image_paths[split], split, \"fine_tuned_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medgemma-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
