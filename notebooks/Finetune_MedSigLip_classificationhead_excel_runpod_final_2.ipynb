{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFU3B09TKuQf"
   },
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Sgv3DCPIA5p-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install --quiet transformers torchvision bitsandbytes datasets evaluate peft trl scikit-learn Pillow ipywidgets jupyterlab_widgets tensorboard sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXh0RtBk_Xc6"
   },
   "source": [
    "## Load model from Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "r-esHCwnQFye"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93ae34a4e404196ba745e57d14275c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/879 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a49f0f886d4b69b558a2818dfe275c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd99413127940d59e8166258050ac3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/360 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a495c384ea884d7ca42692f8518d7eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/809 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b24974633e4b7e83ad4d73df3a888b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11d756789b2474898ff19bb1967c710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/455 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a319ee47cf7481baa37579d64f5a517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.40M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch.nn as nn\n",
    "from transformers import SiglipModel\n",
    "\n",
    "model_id = \"google/medsiglip-448\"\n",
    "\n",
    "class SiglipForClassification(nn.Module):\n",
    "    def __init__(self, siglip_model, num_classes=1):  # num_classes=1 for binary classification\n",
    "        super().__init__()\n",
    "        self.siglip = siglip_model\n",
    "        self.classifier = nn.Linear(siglip_model.config.vision_config.hidden_size, num_classes)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss with logits\n",
    "        \n",
    "    def forward(self, pixel_values, labels=None, **kwargs):\n",
    "        # Get vision embeddings\n",
    "        vision_outputs = self.siglip.vision_model(pixel_values=pixel_values)\n",
    "        pooled_output = vision_outputs.pooler_output\n",
    "        \n",
    "        # Get logits (single output for binary classification)\n",
    "        logits = self.classifier(pooled_output)  # Shape: [batch_size, 1]\n",
    "        \n",
    "        # Compute loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Ensure labels are float and have shape [batch_size, 1]\n",
    "            labels = labels.view(-1, 1).float()  # Reshape and convert to float\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits\n",
    "        }\n",
    "\n",
    "# Load base SigLIP model\n",
    "base_model = SiglipModel.from_pretrained(model_id)\n",
    "model = SiglipForClassification(base_model, num_classes=1)  # Binary classification\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrnDCZvvzWtS"
   },
   "source": [
    "## Prepare fine-tuning dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTYiM4RAjJAo"
   },
   "source": [
    "Load the data using the Hugging Face `datasets` library. Then, create train and validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4602a785da7b4ac8adce812cc16ce99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Appendicitis_vs_normal.zip:   0%|          | 0.00/825M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, Image\n",
    "\n",
    "\n",
    "# 1. Download and Unzip the data from Hub\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# === DOWNLOAD ZIP FILE FROM HUGGING FACE DATASET REPO ===\n",
    "repo_id = \"IraBid-Medical-AI/Appendicitis_vs_normal_classification\"\n",
    "filename = \"Appendicitis_vs_normal.zip\"\n",
    "\n",
    "# Download the ZIP to local cache\n",
    "zip_path = hf_hub_download(\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\",\n",
    "    filename=filename,\n",
    "    revision=\"main\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === UNZIP TO TARGET FOLDER ===\n",
    "extract_dir = \"Appendicitis_vs_normal\"\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(f\"✅ Done: Extracted to {extract_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IPZPx2cxCzxx",
    "outputId": "bd079de6-085b-43ef-85e3-e87cc2255288"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done: Extracted to Appendicitis_vs_normal\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16aeb0d4e2434d848f074c6eb5a805a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7345d5ce964ed39d5dfeff15ff4bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/30450 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b354fe473a4efaa3b3c4ee31d5bd0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing checksums:  47%|####7     | 14429/30450 [00:07<00:07, 2031.16it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef88654a9f3f49e492c4701c716fc387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98316d5ee5b4fb2a994706286c73c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/7630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2553c374bd4ab3988499ce7b64543a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/7630 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343d6d0e02394ff6ba1670e2e72a7caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing checksums:  82%|########1 | 6253/7630 [00:05<00:01, 1250.53it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b89febc05e4438a9e2a91f61104ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6496a8bbd5e34610bc04dca8c2b8f85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/9590 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9f0e39665440398180047ffa2a1db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/9590 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06ff5c738c146e2afd7826b8c9e126c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing checksums:  60%|#####9    | 5730/9590 [00:05<00:03, 1145.22it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfeb8c211d2e4bb7ad22ccbe039fa4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 30450\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 7630\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 9590\n",
      "    })\n",
      "})\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 2. Load dataset\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Path to the extracted dataset folder\n",
    "data_dir = r\"./Appendicitis_vs_normal/Appendicitis_vs_normal\"  # UPDATE this path\n",
    "\n",
    "# Define paths for each split\n",
    "train_dir = f\"{data_dir}/Training_data_png\"\n",
    "val_dir   = f\"{data_dir}/Validation_data_png\"\n",
    "test_dir  = f\"{data_dir}/Test_data_png\"\n",
    "\n",
    "# Load splits using 'imagefolder' format\n",
    "train_ds = load_dataset(\"imagefolder\", data_dir=train_dir, split=\"train\")\n",
    "val_ds   = load_dataset(\"imagefolder\", data_dir=val_dir, split=\"train\")\n",
    "test_ds  = load_dataset(\"imagefolder\", data_dir=test_dir, split=\"train\")\n",
    "\n",
    "# Combine into a DatasetDict\n",
    "data = DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"validation\": val_ds,\n",
    "    \"test\": test_ds,\n",
    "})\n",
    "\n",
    "print(data)\n",
    "\n",
    "# 3. Peek at a sample\n",
    "data[\"train\"][0][\"image\"]\n",
    "# Display the corresponding label\n",
    "print(data[\"train\"][0][\"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V03H6cfnofY_"
   },
   "source": [
    "Inspect a sample data point, which contains:\n",
    "\n",
    "* `image`: image patch as a `PIL` image object\n",
    "* `label`: integer class label corresponding to tissue type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a84a30a964a40f786add428eccffd82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09fab52d634c4b7797700a116219499b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7630 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2e8414b1b448cc81e4126fc8f6ef52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9590 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# adding 'path' column to each dataset\n",
    "\n",
    "def add_path(example, root_dir):\n",
    "    # The 'image' field is a PIL Image, but the dataset from 'imagefolder' also\n",
    "    # includes the 'image' feature with 'path' attribute for each image.\n",
    "    # However, if not present, reconstruct the path from root_dir and label info.\n",
    "    \n",
    "    # Huggingface's imagefolder loader often provides 'path' in 'image' feature metadata,\n",
    "    # So we can try to get it directly:\n",
    "    try:\n",
    "        image_path = example[\"image\"].filename\n",
    "    except AttributeError:\n",
    "        # fallback: try reconstructing (if you know directory structure and filenames)\n",
    "        image_path = None\n",
    "\n",
    "    # If for some reason filename is missing, you can create path from label + file name if available,\n",
    "    # but usually filename should be in the 'image' object.\n",
    "    \n",
    "    return {\"path\": image_path}\n",
    "\n",
    "# Add path column using map for all datasets:\n",
    "train_ds = train_ds.map(lambda x: add_path(x, train_dir))\n",
    "val_ds = val_ds.map(lambda x: add_path(x, val_dir))\n",
    "test_ds = test_ds.map(lambda x: add_path(x, test_dir))\n",
    "\n",
    "# Update the dataset dict\n",
    "data = DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"validation\": val_ds,\n",
    "    \"test\": test_ds,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': Image(mode=None, decode=True), 'label': ClassLabel(names=['negative', 'positive']), 'path': Value('string')}\n"
     ]
    }
   ],
   "source": [
    "print(data[\"train\"].features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "c6EduPmWC4fO",
    "outputId": "1a81aa27-b3df-46cd-b23d-b62a152df041"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/MedGemma/notebooks/Appendicitis_vs_normal/Appendicitis_vs_normal/Training_data_png/negative/0006_0006_slice_007.png'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][287][\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=160x160>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][287][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tKufzKE_C7Vt",
    "outputId": "877f2305-48d2-4a18-c693-22db671acf6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][287][\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHOtCBFfSWyS"
   },
   "source": [
    "## Fine-tune the model\n",
    "\n",
    "Contrastive image-text learning is a method that trains models to learn the relationship between images and text, such that the representations of matching image-text pairs are brought closer together in a shared embedding space while non-matching pairs are pushed further apart.\n",
    "\n",
    "This notebook demonstrates contrastive fine-tuning of MedSigLIP, where the vision and text encoders are jointly trained on image and text data, using the `Trainer` from the Hugging Face `Transformers` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrDwrz6iD3Yl"
   },
   "source": [
    "Define a data collator to prepare batches of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    images = [example[\"image\"] for example in examples]\n",
    "    labels = [example[\"label\"] for example in examples]\n",
    "    paths = [example[\"path\"] for example in examples]  # add paths here\n",
    "    \n",
    "     # Process images\n",
    "    batch = processor(images=images, return_tensors=\"pt\")\n",
    "    \n",
    "    # add labels and paths to the batch\n",
    "    batch[\"labels\"] = torch.tensor(labels, dtype=torch.float).view(-1, 1)\n",
    "    batch[\"path\"] = paths  # return paths as is (list of strings)\n",
    "    \n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "QRPETOTGV81b"
   },
   "outputs": [],
   "source": [
    "#import torch\n",
    "\n",
    "\n",
    "#def collate_fn(examples):\n",
    "    # Extract images and labels\n",
    "#    images = [example[\"image\"] for example in examples]\n",
    "#    labels = [example[\"label\"] for example in examples]\n",
    "    \n",
    "    # Process images\n",
    "#    batch = processor(images=images, return_tensors=\"pt\")\n",
    "    \n",
    "    # Add labels\n",
    "#    batch[\"labels\"] = torch.tensor(labels, dtype=torch.float).view(-1, 1)\n",
    "    \n",
    "#    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fsaon7JoBq5H"
   },
   "source": [
    "Configure training parameters in [`TrainingArguments`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "E6W_gQmfRXWx"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "num_train_epochs = 3  # @param {type: \"number\"}\n",
    "learning_rate = 1e-5  # @param {type: \"number\"}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"medsiglip-appendicitis-binary\",  # Directory and Hub repository id to save the model to\n",
    "    num_train_epochs=num_train_epochs,      # Number of training epochs\n",
    "    per_device_train_batch_size=8,          # Batch size per device during training\n",
    "    per_device_eval_batch_size=8,           # Batch size per device during evaluation\n",
    "    gradient_accumulation_steps=4,          # Number of steps before performing a backward/update pass\n",
    "    logging_steps=50,                       # Number of steps between logs\n",
    "    save_strategy=\"epoch\",                  # Save checkpoint every epoch\n",
    "    eval_strategy=\"steps\",                  # Evaluate every `eval_steps`\n",
    "    eval_steps=100,                          # Number of steps between evaluations\n",
    "    learning_rate=learning_rate,            # Learning rate\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,  # Better than warmup_steps=1                 \n",
    "    lr_scheduler_type=\"cosine\", \n",
    "    remove_unused_columns=False,            # Use cosine learning rate scheduler\n",
    "    push_to_hub=True,                       # Push model to Hub\n",
    "    report_to=\"tensorboard\",                # Report metrics to tensorboard\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8560a894914e42d1adb5102abd2f9a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a3e8f32e694c368804f42c0a1e9496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4dc53ca9758424da9bf197403168ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "roc_auc_metric = evaluate.load(\"roc_auc\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute metrics for binary classification (normal vs. positive).\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred  # predictions: [batch_size, 1], labels: [batch_size]\n",
    "    \n",
    "    # Convert logits to probabilities and predictions\n",
    "    probabilities = 1 / (1 + np.exp(-predictions))  # Sigmoid: [batch_size, 1]\n",
    "    pred_classes = (probabilities > 0.5).astype(np.int32).flatten()  # Threshold at 0.5: [batch_size]\n",
    "    \n",
    "    # Ensure labels are integers\n",
    "    labels = labels.astype(np.int32)\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = {}\n",
    "    metrics.update(accuracy_metric.compute(\n",
    "        predictions=pred_classes,\n",
    "        references=labels,\n",
    "    ))\n",
    "    metrics.update(f1_metric.compute(\n",
    "        predictions=pred_classes,\n",
    "        references=labels,\n",
    "        average =\"macro\"\n",
    "    ))\n",
    "    # Compute ROC-AUC using positive class probabilities\n",
    "    try:\n",
    "        auc = roc_auc_metric.compute(\n",
    "            prediction_scores=probabilities.flatten(),  # Positive class probabilities\n",
    "            references=labels,\n",
    "            average=\"macro\"  # Binary classification uses macro for consistency\n",
    "        )\n",
    "        metrics.update({\"auc\": auc[\"roc_auc\"]})\n",
    "    except Exception as e:\n",
    "        metrics.update({\"auc\": None, \"auc_error\": str(e)})\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1rMdSj1Tj1K"
   },
   "source": [
    "Construct a [`Trainer`](https://huggingface.co/docs/transformers/trainer) using the previously defined training parameters and data collator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "dWcynpm0MHDc"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"].shuffle().select(range(100)),  # Use subset of validation set for faster run\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoHlpSKKVbDb"
   },
   "source": [
    "Launch the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VZcOXgddDSRd",
    "outputId": "c31b7685-4f44-4be7-af97-d41778f43736"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 04:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.649401</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.397590</td>\n",
       "      <td>0.504456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.580500</td>\n",
       "      <td>0.649753</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.397590</td>\n",
       "      <td>0.529412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.580500</td>\n",
       "      <td>0.636040</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.397590</td>\n",
       "      <td>0.561497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.553700</td>\n",
       "      <td>0.637064</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.397590</td>\n",
       "      <td>0.566845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=28, training_loss=2.50087308883667, metrics={'train_runtime': 256.0739, 'train_samples_per_second': 3.397, 'train_steps_per_second': 0.109, 'total_flos': 0.0, 'train_loss': 2.50087308883667, 'epoch': 2.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wf9QkPFAfegp"
   },
   "source": [
    "## Evaluate the fine-tuned model on a classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 435\n",
      "Evaluating model on train dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 14/14 [00:32<00:00,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "EVALUATION RESULTS\n",
      "==================================================\n",
      "Overall Accuracy: 0.7333\n",
      "Weighted F1-Score: 0.7003\n",
      "Macro F1-Score: 0.6428\n",
      "\n",
      "Per-Class Metrics:\n",
      "----------------------------------------------------------------------\n",
      "Class           Precision    Recall       F1-Score     Support   \n",
      "----------------------------------------------------------------------\n",
      "normal          0.7330       0.9373       0.8226       287       \n",
      "appendicitis    0.7353       0.3378       0.4630       148       \n",
      "----------------------------------------------------------------------\n",
      "Weighted Avg    0.7338       0.7333       0.7003       435       \n",
      "Macro Avg       0.7341       0.6376       0.6428       435       \n",
      "\n",
      "Detailed Classification Report:\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal     0.7330    0.9373    0.8226       287\n",
      "appendicitis     0.7353    0.3378    0.4630       148\n",
      "\n",
      "    accuracy                         0.7333       435\n",
      "   macro avg     0.7341    0.6376    0.6428       435\n",
      "weighted avg     0.7338    0.7333    0.7003       435\n",
      "\n",
      "✅ Results saved to train_predictions.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_recall_fscore_support, \n",
    "    classification_report, \n",
    "    confusion_matrix\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(model, train_dataset, processor, batch_size=32, device='cuda'):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of the fine-tuned model on train data\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    all_paths = []\n",
    "    print(\"Evaluating model on train dataset...\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Testing\")):\n",
    "            if 'pixel_values' not in batch or 'labels' not in batch:\n",
    "                raise ValueError(\"Batch missing required keys: 'pixel_values' or 'labels'.\")\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            paths = batch['path']\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            logits = outputs['logits']\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "            predictions = (probabilities > 0.4).long().view(-1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_logits.extend(probabilities.cpu().numpy().flatten())  # Save probabilities\n",
    "            all_paths.extend(paths)\n",
    "    return np.array(all_predictions), np.array(all_labels), np.array(all_logits), all_paths\n",
    "\n",
    "\n",
    "def compute_detailed_metrics(predictions, labels, class_names):\n",
    "    \"\"\"\n",
    "    Compute detailed evaluation metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        labels, predictions, average=None\n",
    "    )\n",
    "    \n",
    "    # Weighted averages\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted'\n",
    "    )\n",
    "    \n",
    "    # Macro averages\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='macro'\n",
    "    )\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Weighted F1-Score: {f1_weighted:.4f}\")\n",
    "    print(f\"Macro F1-Score: {f1_macro:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print(\"Per-Class Metrics:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"{class_name:<15} {precision[i]:<12.4f} {recall[i]:<12.4f} {f1[i]:<12.4f} {support[i]:<10}\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Weighted Avg':<15} {precision_weighted:<12.4f} {recall_weighted:<12.4f} {f1_weighted:<12.4f} {sum(support):<10}\")\n",
    "    print(f\"{'Macro Avg':<15} {precision_macro:<12.4f} {recall_macro:<12.4f} {f1_macro:<12.4f} {sum(support):<10}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision_weighted': precision_weighted,\n",
    "        'recall_weighted': recall_weighted,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'per_class_precision': precision,\n",
    "        'per_class_recall': recall,\n",
    "        'per_class_f1': f1,\n",
    "        'support': support\n",
    "    }\n",
    "\n",
    "\n",
    "def run_evaluation():\n",
    "    ABDOMINAL_PAIN_CLASSES = ['normal', 'appendicitis']\n",
    "    train_data = data[\"train\"]\n",
    "    print(f\"Train dataset size: {len(train_data)}\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    predictions, labels, probabilities, paths = evaluate_model(\n",
    "        model, \n",
    "        train_data, \n",
    "        processor, \n",
    "        batch_size=32, \n",
    "        device=device\n",
    "    )\n",
    "    metrics = compute_detailed_metrics(predictions, labels, ABDOMINAL_PAIN_CLASSES)\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(\"=\"*50)\n",
    "    print(classification_report(\n",
    "        labels, \n",
    "        predictions, \n",
    "        target_names=ABDOMINAL_PAIN_CLASSES,\n",
    "        digits=4\n",
    "    ))\n",
    "    # Save results to Excel\n",
    "    df_results = pd.DataFrame({\n",
    "        \"image_path\": paths,\n",
    "        \"probability\": probabilities,\n",
    "        \"actual_label\": labels.flatten(),\n",
    "        \"prediction\": predictions.flatten()\n",
    "    })\n",
    "    df_results.to_excel(\"train_predictions.xlsx\", index=False)\n",
    "    print(\"✅ Results saved to train_predictions.xlsx\")\n",
    "    return metrics, predictions, labels, probabilities, paths\n",
    "\n",
    "# Run the evaluation\n",
    "evaluation_results = run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_recall_fscore_support, \n",
    "    classification_report, \n",
    "    confusion_matrix\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(model, valid_dataset, processor, batch_size=32, device='cuda'):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of the fine-tuned model on valid data\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    all_paths = []\n",
    "    print(\"Evaluating model on valid dataset...\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(valid_loader, desc=\"Testing\")):\n",
    "            if 'pixel_values' not in batch or 'labels' not in batch:\n",
    "                raise ValueError(\"Batch missing required keys: 'pixel_values' or 'labels'.\")\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            paths = batch['path']\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            logits = outputs['logits']\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "            predictions = (probabilities > 0.4).long().view(-1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_logits.extend(probabilities.cpu().numpy().flatten())  # Save probabilities\n",
    "            all_paths.extend(paths)\n",
    "    return np.array(all_predictions), np.array(all_labels), np.array(all_logits), all_paths\n",
    "\n",
    "\n",
    "def compute_detailed_metrics(predictions, labels, class_names):\n",
    "    \"\"\"\n",
    "    Compute detailed evaluation metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        labels, predictions, average=None\n",
    "    )\n",
    "    \n",
    "    # Weighted averages\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted'\n",
    "    )\n",
    "    \n",
    "    # Macro averages\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='macro'\n",
    "    )\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Weighted F1-Score: {f1_weighted:.4f}\")\n",
    "    print(f\"Macro F1-Score: {f1_macro:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print(\"Per-Class Metrics:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"{class_name:<15} {precision[i]:<12.4f} {recall[i]:<12.4f} {f1[i]:<12.4f} {support[i]:<10}\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Weighted Avg':<15} {precision_weighted:<12.4f} {recall_weighted:<12.4f} {f1_weighted:<12.4f} {sum(support):<10}\")\n",
    "    print(f\"{'Macro Avg':<15} {precision_macro:<12.4f} {recall_macro:<12.4f} {f1_macro:<12.4f} {sum(support):<10}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision_weighted': precision_weighted,\n",
    "        'recall_weighted': recall_weighted,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'per_class_precision': precision,\n",
    "        'per_class_recall': recall,\n",
    "        'per_class_f1': f1,\n",
    "        'support': support\n",
    "    }\n",
    "\n",
    "\n",
    "def run_evaluation():\n",
    "    ABDOMINAL_PAIN_CLASSES = ['normal', 'appendicitis']\n",
    "    valid_data = data[\"valid\"]\n",
    "    print(f\"Validati dataset size: {len(valid_data)}\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    predictions, labels, probabilities, paths = evaluate_model(\n",
    "        model, \n",
    "        test_data, \n",
    "        processor, \n",
    "        batch_size=32, \n",
    "        device=device\n",
    "    )\n",
    "    metrics = compute_detailed_metrics(predictions, labels, ABDOMINAL_PAIN_CLASSES)\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(\"=\"*50)\n",
    "    print(classification_report(\n",
    "        labels, \n",
    "        predictions, \n",
    "        target_names=ABDOMINAL_PAIN_CLASSES,\n",
    "        digits=4\n",
    "    ))\n",
    "    # Save results to Excel\n",
    "    df_results = pd.DataFrame({\n",
    "        \"image_path\": paths,\n",
    "        \"probability\": probabilities,\n",
    "        \"actual_label\": labels.flatten(),\n",
    "        \"prediction\": predictions.flatten()\n",
    "    })\n",
    "    df_results.to_excel(\"test_predictions.xlsx\", index=False)\n",
    "    print(\"✅ Results saved to test_predictions.xlsx\")\n",
    "    return metrics, predictions, labels, probabilities, paths\n",
    "\n",
    "# Run the evaluation\n",
    "evaluation_results = run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset size: 137\n",
      "Evaluating model on test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 5/5 [00:10<00:00,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "EVALUATION RESULTS\n",
      "==================================================\n",
      "Overall Accuracy: 0.6350\n",
      "Weighted F1-Score: 0.5716\n",
      "Macro F1-Score: 0.4903\n",
      "\n",
      "Per-Class Metrics:\n",
      "----------------------------------------------------------------------\n",
      "Class           Precision    Recall       F1-Score     Support   \n",
      "----------------------------------------------------------------------\n",
      "normal          0.6612       0.8989       0.7619       89        \n",
      "appendicitis    0.4375       0.1458       0.2188       48        \n",
      "----------------------------------------------------------------------\n",
      "Weighted Avg    0.5828       0.6350       0.5716       137       \n",
      "Macro Avg       0.5493       0.5224       0.4903       137       \n",
      "\n",
      "Detailed Classification Report:\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal     0.6612    0.8989    0.7619        89\n",
      "appendicitis     0.4375    0.1458    0.2188        48\n",
      "\n",
      "    accuracy                         0.6350       137\n",
      "   macro avg     0.5493    0.5224    0.4903       137\n",
      "weighted avg     0.5828    0.6350    0.5716       137\n",
      "\n",
      "✅ Results saved to test_predictions.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_recall_fscore_support, \n",
    "    classification_report, \n",
    "    confusion_matrix\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(model, test_dataset, processor, batch_size=32, device='cuda'):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of the fine-tuned model on test data\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    all_paths = []\n",
    "    print(\"Evaluating model on test dataset...\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(test_loader, desc=\"Testing\")):\n",
    "            if 'pixel_values' not in batch or 'labels' not in batch:\n",
    "                raise ValueError(\"Batch missing required keys: 'pixel_values' or 'labels'.\")\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            paths = batch['path']\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            logits = outputs['logits']\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "            predictions = (probabilities > 0.4).long().view(-1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_logits.extend(probabilities.cpu().numpy().flatten())  # Save probabilities\n",
    "            all_paths.extend(paths)\n",
    "    return np.array(all_predictions), np.array(all_labels), np.array(all_logits), all_paths\n",
    "\n",
    "\n",
    "def compute_detailed_metrics(predictions, labels, class_names):\n",
    "    \"\"\"\n",
    "    Compute detailed evaluation metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        labels, predictions, average=None\n",
    "    )\n",
    "    \n",
    "    # Weighted averages\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted'\n",
    "    )\n",
    "    \n",
    "    # Macro averages\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='macro'\n",
    "    )\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Weighted F1-Score: {f1_weighted:.4f}\")\n",
    "    print(f\"Macro F1-Score: {f1_macro:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print(\"Per-Class Metrics:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"{class_name:<15} {precision[i]:<12.4f} {recall[i]:<12.4f} {f1[i]:<12.4f} {support[i]:<10}\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Weighted Avg':<15} {precision_weighted:<12.4f} {recall_weighted:<12.4f} {f1_weighted:<12.4f} {sum(support):<10}\")\n",
    "    print(f\"{'Macro Avg':<15} {precision_macro:<12.4f} {recall_macro:<12.4f} {f1_macro:<12.4f} {sum(support):<10}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision_weighted': precision_weighted,\n",
    "        'recall_weighted': recall_weighted,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'per_class_precision': precision,\n",
    "        'per_class_recall': recall,\n",
    "        'per_class_f1': f1,\n",
    "        'support': support\n",
    "    }\n",
    "\n",
    "\n",
    "def run_evaluation():\n",
    "    ABDOMINAL_PAIN_CLASSES = ['normal', 'appendicitis']\n",
    "    test_data = data[\"test\"]\n",
    "    print(f\"Test dataset size: {len(test_data)}\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    predictions, labels, probabilities, paths = evaluate_model(\n",
    "        model, \n",
    "        test_data, \n",
    "        processor, \n",
    "        batch_size=32, \n",
    "        device=device\n",
    "    )\n",
    "    metrics = compute_detailed_metrics(predictions, labels, ABDOMINAL_PAIN_CLASSES)\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(\"=\"*50)\n",
    "    print(classification_report(\n",
    "        labels, \n",
    "        predictions, \n",
    "        target_names=ABDOMINAL_PAIN_CLASSES,\n",
    "        digits=4\n",
    "    ))\n",
    "    # Save results to Excel\n",
    "    df_results = pd.DataFrame({\n",
    "        \"image_path\": paths,\n",
    "        \"probability\": probabilities,\n",
    "        \"actual_label\": labels.flatten(),\n",
    "        \"prediction\": predictions.flatten()\n",
    "    })\n",
    "    df_results.to_excel(\"test_predictions.xlsx\", index=False)\n",
    "    print(\"✅ Results saved to test_predictions.xlsx\")\n",
    "    return metrics, predictions, labels, probabilities, paths\n",
    "\n",
    "# Run the evaluation\n",
    "evaluation_results = run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "fine_tune_with_hugging_face.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.11.3 (MedGemma)",
   "language": "python",
   "name": "medgamma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
